---
layout: post
title: Advertisement Insight and User Patterns with Last.fm
subtitle: Data Analysis Project - Part 1
comments: true
gh-repo: awilde27/Last.fm-Project
gh-badge: [star, watch, follow]
image: /imgs/lastfmlogo.png
---

The [Last.fm Dataset - 1K users](http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html), taken from Òscar Celma's [webpage](http://ocelma.net/), consists of entire listening habits for nearly 1,000 users. I became interested in this dataset because of its raw nature, which meant I would be able to spend a lot of time trying to manipulate the data in quantifiable and insightful ways.

1. [Abstract](#abstract)
2. [Intro](#intro)
3. [Data Processing](#data-processing)
4. [Algorithms](#algorithms)
5. [Conclusions](#conclusions)
6. [Takeaways](#takeaways)

## Abstract

This project was a combination of exploratory data analysis, feature engineering and prediction with the goal of understanding user behavior with respect to their music choices. Music streaming data has the potential to hold many secrets about user listening habits, for example whether users have any hierarchical patterns in popularity of the songs they choose. Through data analysis of Last.fm's music streaming data for 63 chosen US users, we can gauge a user's tendencies on the manner in which the user subconsciously chooses a sequence of songs during a listening period. For a fixed song amount in a given user listening session, there is evidence to suggest a subconscious (or conscious) decision making process for choosing songs related to the distribution of popularity among songs and artists the user has listened to in the past. This directly leads to a possible advertisement scheme, one which takes into account the likelihood of the user remaining on the music app while waiting for an ad to play. To avoid textual analysis, one can look for similarities among users after frequencies (popularities) are normalized for each user. However, the findings of this analysis seemed to show little pattern on large amounts of listening periods.

Full write up with commentary, code and visualizations is below.

## Intro

![User Map](/imgs/User_map_notags.png)

This dataset proved to be _large_ (2.2GB), which lead me to choose all US users with known age and gender. My initial goals of the project were to somehow transform the time series data into something that could be easily interpreted so that I could garner insights for a potential advertisement structure or find patterns among users. As the project progressed, I realized how much the data really could dictate the quantity and quality of insights and ultimately the usability of one’s findings. 


The Last.fm 1K User dataset consists of **1.2 million rows**, each having a timestamp, user, artist/song ID, and artist/song name. Within the context of this project, all of the information from the artist and song ID’s were encapsulated in the artist and song names, so they were dropped from the dataset. 

| User | Timestamp | Artist Name | Track Name |
| :---- | :------ | :------ | :------ |
| **user_000003** | 2009-05-04 10:29:04 | Samuli Kemppi | Samuli Kemppi - January 2009 |
| **user_000003** | 2009-04-21 16:56:35 | Dirt Nasty | True Hollywood Story |
| **user_000003** | 2009-04-21 16:47:11 | Dirt Nasty | Gotta Leave This Town |
| **user_000003** | 2009-04-21 16:32:54 | Mickey Avalon | F'in Em All |
| **user_000003** | 2009-04-21 16:27:08 | Daft Punk | Daftendirekt |

{: .box-note}
**Side Note:** This is where NLP would have come in handy... 


Somewhat unfortunately, excluding the users and their background information, the dataset only has three features. From a machine learning perspective, this is a bit problematic especially since song and artists are textual features. 

The next step in the stream of thought would perhaps be to perform some brute force machine learning algorithm using Natural Language Processing. This would immediately be able to provide strong insights towards user-user similarities from sentiment analysis; there may be a strong likelihood for a successful k-Nearest Neighbors algorithm in the song and artist spaces. However, NLP is a bit out of my scope so I decided to avoid this route for now (_I will see to it that I learn NLP and come back to this ASAP, however_). Consequently, the full user patterns part of this project has not yet developed.

I brainstormed for some time on how I could manipulate the data to end up with numerical, descriptive features without losing too much information. First thing that came to mind was frequencies as well as temporal patterns. My intuition told me that regression and random forests could be good models with this kind of data and the right manipulation.



{% highlight python linenos %}
##Converting the time stamp into date and time
us_data['Timestamp'] = us_data['Timestamp'].str.replace('Z', '')
master = us_data

user_dfs = []
for user in us_users:
    user_dfs.append(master.loc[user].astype({'Timestamp':'datetime64[s]'}))

user_dfs[0].head()
{% endhighlight %}

## Data Processing

{: .box-warning}
**Note:** Throughout the data manipulation with frequency data, I did not combine users’ data together, so as to not mix the data among user data points. Also, I have included tables for the first user in the list, User 3 (pulled from all 992 users).

At the beginning of the journey through transforming the data, I wanted to find the top frequencies with respect to both songs and artists. Isolating the artist or song column using the value counts function in Pandas creates a new series with appearance counts for each artist or song. 

{% highlight python linenos %}
def top_freq_song(df):
    ''' 
    Computes frequencies of each unique row label in df and outputs all rows 
    with value greater than the chop value, and dropping the resulting NaNs.
    Input: Timestamped DataFrame
    Output: Frequency Series 
    Note: Since this function takes the whole series as an argument, 
    I can't use the apply function here.
    '''
    frequencies = df.value_counts()
    if len(frequencies) < 500:
        chop_val = 0
    elif 500 < len(frequencies) < 1000:
        chop_val = 3
    elif 1000 < len(frequencies) < 2350:
        chop_val = 4
    elif 2350 < len(frequencies) < 8000:
        chop_val = 5
    else:
        chop_val = 10
    return frequencies.where(frequencies >= chop_val).dropna()
{% endhighlight %}

```python
def top_freq_artist(df):
    frequencies = df.value_counts()
    chop_val = np.round(len(frequencies)*.01)
    return frequencies.where(frequencies >= chop_val).dropna()
```

{: .box-warning}
**A Little Heads-up:** I chopped off the lowest frequency values depending on the hyperparameter _chop value_, so as not to delete too many entries from the original data but keep the songs and artists with non-trivial frequencies. Motivation behind this was that the lower frequency songs, for example, correspond to the user listening to those songs very sparingly over years of listening. However, I don't think my conclusions would have differed much had I kept these.

### Top 10 Artists per Age Group

![Top Artists by Age](/imgs/top-artist-age.png)

### Top Artist and Song Plays for User 3

![Top songs user 3](/imgs/user3-top-songs08.png) ![Top arists user 3](/imgs/user3-top-artists08.png)


Next, I concatenate the song and artist time series to get the DataFrame back to normal then reverse the ordering so that the earliest timestamp is now the first row. 

{% highlight python linenos %}
def user_dataframer(song_ts, artist_ts):
    '''
    Concatenates each user's data back into a 2-column data frame.  
    I also reverse the ordering so that the earliest day is row 1.
    '''
    df = pd.concat([artist_ts, song_ts], axis = 1, 
    				keys=['Artist', 'Song']).iloc[::-1]
    return df
{% endhighlight %}

Following this, I filter the DataFrames on the top artists or songs that were found from the top-freq-(artist/song) function. Finally, I add a new column which maps the song or artist frequencies to their respective song or artist. This creates a new numerical feature, total frequency.

{% highlight python linenos %}
def filter_time_series(df, filtering_ts, filter_by):
    '''
    Takes a song time series and an artist time series, concatenates 
    and reverses using user_dataframer function. Then filters on 
    'filter_by' with 'frequencies' which convert to filter_list.
    '''
    filter_list = list(filtering_ts.index)
    return df.loc[df[filter_by].isin(filter_list)]
{% endhighlight %}

Having “engineered” a numerical feature, the question becomes: in what direction do I keep moving? Trying to build an advertisement placement optimization model with just frequencies seems difficult and not useful. However, I _might_ be able to discover some insight if I looked for a potential relationship between time a song or artist was played (locally within the data) and its total frequency. My objective thus became strategically splitting the data into smaller chunks with respect to time.

**The idea is as follows**: if we can predict when a user is likely to play a popular song or artist, then this user will probably be more likely to accept listening to an advertisement right before the song is played. I argue that this is because the potential excitement or anticipation this person may have could create just enough tolerance to accept listening to an advertisement. 

_This notion relies on a strong assumption, though from a naïve perspective it is definitely worth pursuing_. In reality, given the nature of radios like Last.fm, this scenario is not even possible. In other words, a user must actively pick a song to play next from a library, warranting this idea null since the prediction becomes irrelevant. However, with a slight tweak – where after a period of songs played with lower frequencies a popular song is suddenly chosen – we can put an advertisement in right before this popular song is played. This is a similar, yet much more realistic approach. Regardless, I chose to proceed by splitting the data into temporal chunks. 

{% highlight python linenos %}
def get_artist_frequency_tuples(df):
    timedeltas = []
    for i in range(len(df)):
        if i == 0:
            timedeltas.append(0)
        else:
            timedeltas.append((df.index[i] - df.index[i-1]).seconds)
    td_arr = np.array(timedeltas)
    splits = np.argwhere(td_arr > 600).flatten() #find indices where periods of listening occured
    tuples = []
    for i in range(len(splits)):
        if i == 0:
            tuples.append(df['Artist_Freq'][:splits[i]])
        elif i == len(splits)-1:
            tuples.append(df['Artist_Freq'][splits[i]:len(df)])
        else:
            tuples.append(df['Artist_Freq'][splits[i]:splits[i+1]])
    assert len(tuples) == len(splits), "Something wrong in the indices"
    return tuples
{% endhighlight %}

This functiontakes a DataFrame and splitws it into smaller frames, corresponding to periods of listening for a given user. I assumed that inactivity for over 10 minutes implied that the person had finished a listening session.

For get-frequency-tuples, splitting up the data chunks did call for hyperparameters. This hyperparameter was:
> 1. Length of cut frames (chunk size)  
	- There technically is no ideal chunk size, as a period of listening can uniformly vary in terms of the number of songs played.

{% highlight python linenos %}
def get_final_freq(tuple_list, chunk_size):
    data = []
    outcomes = []
    for i in range(len(tuple_list)):
        if len(tuple_list[i]) == chunk_size:
            data.append(np.array(tuple_list[i][:chunk_size-1]))
            outcomes.append(np.array(tuple_list[i][chunk_size-1]))
    return np.array(data), np.array(outcomes)
{% endhighlight %}

In order to perform algorithms on the finally transformed data, I needed to ensure the frames were all cut with the same length. I was a bit unsure about how I could work around this since the DataFrames came in different lengths due to the number of intersections they had. The length parameter of these DataFrames is extremely important: the length determines how many songs can be played before an advertisement is deployed.  Nevertheless, it appears that any length yielding a good amount of data seems viable, so at the price of potentially losing some information I decided to choose the modal length for a given user. However, another issue I encountered was the fact that the median lengths were different among users but this didn’t cause problems since my algorithms were implemented on each specific user.

After I split up the DataFrame into chunks either of length four (no original intersection) or the modal length of all intersecting chunks, I put them into array/matrix form. 

## Algorithms

Although implementing algorithms are usually straightforward with a clean dataset with many features, in this project I found it quite difficult and frustrating to develop sound insight from my manipulated dataset. The reasons behind this were such:

##### 1. Features  

Anyone can see that the features present actually come from the decomposition of the individual frames. This lead to a vague representation of the feature space and highly dependent variables.

##### 2. The nature of the frequency feature 

This feature is in fact a summarizing feature, which means a lot of information is lost from the original data. This sort of broad brushing ended up being a major pitfall in trying to obtain high accuracies in my predictions.



I decided in the spirit of the advertisement optimization to attempt to predict the “popularity” or frequency of the song at the end of a period of listening. A human picking a procession of songs potentially will have no inherent pattern. However, an accuracy of about 75% or higher would probably imply that the model of choice could illuminate a statistically significant correlation from the first songs to the last.

### Algorithms on a Chunk Size of 4, Artist Frequencies

 ----


#### Simple Linear Regression
----
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LassoCV

def lin_reg_fit(X, y):
    '''
    Fits a simple linear regression model given input X and output y.
    '''
    return np.linalg.inv(X.T @ X).dot(X.T @ y)

def avg_mse(X, w, y):
    '''
    Finds the average mean squared error from solving linear regression
    '''
    predicted = X.dot(w)
    real = y
    return np.linalg.norm(predicted - real)**2 / len(real)
```

----

##### Best Linear Regression Training Error

----

```
avg_mse(x4_artist[42], lin_reg_fit(x4_artist[42], y4_artist[42]), y4_artist[42])
>>> 35.50240828
```

----

##### Top 10 Training Errors for Linear Regression

![Lin Reg](/imgs/Top10_LR_artist.png)

----

#### Polynomial Regression

```python
def poly_reg_fit(X_train, y_train):
    pf = PolynomialFeatures(include_bias=True)
    poly_X = pf.fit_transform(X_train)
    
    w = np.linalg.inv(poly_X.T.dot(poly_X)).dot(poly_X.T.dot(y_train))
    
    return np.linalg.norm(poly_X @ w - y_train)**2 / len(y_train)
```

----

##### Best Polynomial Regression Training Error
----
``` 
poly_reg_fit(x4_artist[42], y4_artist[42])
>>> 22.13509843
```
----

##### Top 10 Training Errors for Polynomial Regression

![Poly Reg Scores](/imgs/Top10_PR_artist.png)

----

### Ridge Regression
____

##### Best Ridge R^2, alpha = 1

____

```
ridge_score(x4_artist[42], y4_artist[42])
>>> 0.99773
```

____

##### Top 10 Training Errors for Ridge Regression

![Ridge R2](/imgs/Top10_ridgeR_artist.png)

____

#### Lasso Regression

----

```python
def lasso_noCV(X_train, y_train):
    lasso = Lasso(alpha=1)
    lasso.fit(X_train, y_train)
    return lasso.score(X_train, y_train)
```

----

##### Best Lasso R^2, alpha = 1

----

```
lasso_noCV(x4_artist[42], y4_artist[42])
>>> 0.99773
```

____

##### Top 10 Training Errors for Lasso Regression

![Lasso R2](/imgs/Top10_lassoR_artist.png)

____


#### Lasso Regression with Cross-Validation
----
```python
def lassoCV_mse(X_train, y_train):
    lasso = LassoCV(alphas=np.arange(2, 15, 1))
    lasso.fit(X_train, y_train)
    return lasso.mse_path_
```

----

##### Best Lasso Train Error for 3-Fold Cross Validation, varying alphas
----
``` 
lasso_fit(x4_artist[42], y4_artist[42])
alphas = [2, 3, 4, 5, 6, 7, 8, 9,
		 10, 11, 12, 13, 14]
>>> [[ 139.89911613,   24.11027753,    1.27922479],
	[ 144.44427631,   24.09595757,    1.27396024],
	[ 159.95069467,   24.08147317,    1.27064801],
	[ 186.36383239,   24.06683327,    1.26716457],
	[ 223.72969872,   24.05204649,    1.26374209],
	[ 272.03755452,   24.03489591,    1.26043836],
	[ 331.27853926,   24.00977758,    1.2572632 ],
	[ 401.4416292 ,   23.98421206,    1.25421358],
	[ 482.66433246,   23.95767281,    1.25128331],
	[ 574.71267374,   23.93007466,    1.24846578],
	[ 677.62916009,   23.90145228,    1.24607874],
	[ 791.64719035,   23.87224077,    1.24318051],
	[ 916.39470862,   23.84496151,    1.24035094]]
```


It turns out that Ridge Regression and Lasso Regression both had basically the same performance, indicating that there was no better regularization penalty between L1 and L2 norms.


#### Random Forest

I ended up running a Random Forest regressor with an L1 norm (mean absolute error) loss to measure the quality of a split well as an L1 norm regularization penalty for Lasso regression. The reasoning behind choosing an L1 loss is because the errors will naturally be high given the large variance within features for each sample, so a squared loss like the L2 norm can get large if an individual error is quite large.

I chose to report the Out-of-Bag error for this random forest regressor. The OOB estimate is as accurate as using a test set the same size as the training set, according to [Breiman (1996b)](https://www.stat.berkeley.edu/users/breiman/OOBestimation.pdf). Random forest techniques involve sampling of the input data with replacement, i.e. bootstrap sampling. In this sampling, roughly 1/3 of the data -- the out of bag samples --  isn't used and can be used for testing.

```python
# Framework for implementing a Random Forest Classifier

def random_forest_reg(X_train, y_train, X_test, y_test):
    '''
    Random forest regressor using mean absolute error for objective function.
    '''
    
        RF = RandomForestRegressor(max_depth=4, max_features=0.33, criterion='mae', oob_score=True)
    RF.fit(X_train, y_train)

    return RF.oob_score_
```

----

##### Best OOB R^2 Score
----
``` 
random_forest_reg(x4_artist[42], y4_artist[42])
>>> 0.83028118
```
----

##### Top 10 Best OOB R^2 Scores

![RF Scores](/imgs/Top10_RF_artist.png)


### Adaptive Binning Classification

My next thought would be to put the frequencies into adaptive bins, and try to classify which bin the last song would appear in. That is, I chose bins whose measurements were quantiles of song or artist frequencies. In the context of this project, it makes slightly more sense to consider song frequencies rather than artist frequencies. Because songs in a period of listening have a strong interaction with one another, I chose polynomial regression as my primary model. This model would be able to tell me whether certain songs or combinations of songs in the period have a high impact on the prediction of the final song frequency bin. 

On a more complex scale, this idea can be combined with a song recommendation methodology. For each song, instead of recommending songs based on a general procedure, the procedure can be modified to pick songs similar in frequency to the frequency of the next predicted song.

I chose to use a Random Forest Classifier with the adaptive bins as the distinct classes.


## Conclusions

After the models were trained and scored, it seemed that the random forest regressor performed the best, followed by Lasso, Polynomial and Linear Regression in last place. This is pretty intuitive as the model complexity increased, and Lasso shold have performed better because 
As said before, I was not expecting great results from the algorithms given the nature of my features. However, the Random Forest Regressor did seem to do just fine for a few of the users. The large variance of scores could imply that some users are more predictable than others when it comes to picking songs on the Last.fm radio. 

Given the large amount of poor errors for every machine learning model, it is reasonable to conclude that users are not overly predictable with their methodology of choosing songs concurrently. As a result, the original hypothesis that an user will likely play a popular song after a period of songs not as popular does not have statistically significant evidence to be true. That being said, the presuming assumption preceding implementing algorithms was that this popular song came at the very end of a listening period, which could also be proved false.

Ultimately, this part of the project does shed insight on the importance of picking good factors for optimizing an advertising schema. 

{: .box-note}
**Regardless of these outcomes, I am firm in my belief that an advertisement structure for a radio must be much more than playing an advertisement every certain number of songs. On a radio platform like Last.fm or Pandora, user retention, i.e. listening session time maximization, is crucial for revenue generation and overall company strength.**




## Takeaways
First thing's first, there were definitely a lot of flaws in this project.

**Here are some noteworthy ones** 

1. Not considering the entire dataset. This could affect algorithm-driven conclusions on a macro scale.
2. Trying to implement algorithms one essentially one feature, frequencies. As you can see from this project, I ended up making each song in the dataframe a pseudo-feature, which clearly has its flaws since the song plays are **highly** collinear.  
3. Regarding my algorithms, my input data implicates that the predictions should occur on the final song of the listening period. This is not the same as simply observing instances in the data where there is a spike in frequencies. As a result, this is a big issue that I should have addressed more.
4. On a similar note, there was a weird dynamic having multiple users. If I want to examine each user individually, which user do I train with? Which user do I test with? Do I test with multiple users, or should I train with multiple users and then test with one? These questions made the data seem pretty confusing.
5. Potential for misleading insights. When data manipulation comes into play, there is always a possibility. Even if my accuracies had been high, this shouldn't have been an indication that the ad structure was necessarily usable.


All in all, I leaned a lot from this project. The moral of the story is contained in the data.  Having one dataset can be a bit restricting. Of course, there are plenty of datasets and APIs out there related to this Last.fm dataset that I could have taken advantage of. In the end, data almost always has full dictatorship over the destinations at the end of your journey, as well as the journey itself.


#### Thoughts on this project? Feel free to post a comment below. Thanks for reading :)
